{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Classification\n",
    "\n",
    "In this lab, we are going to build a classification module. When given an image of a handwritten digit like the one below, the model will be able to tell which digit is in the image.\n",
    "\n",
    "<img src='test2.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# Depending on library versions on your system, one of the following imports \n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = r'digits_dataset'\n",
    "target_img_size = (32, 32) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "\n",
    "# We are going to fix the random seed to make our experiments reproducible \n",
    "# since some algorithms use pseudorandom generators\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Feature Extraction\n",
    "\n",
    "In this part, we are going to implement three functions. Each one will extract a different set of features from the image. The three sets are:\n",
    "\n",
    "1. Histogram of the pixel values features (this is the histogram you know, but on the HSV channels)\n",
    "2. Histogram of Gradients (HoG) features\n",
    "3. Raw pixels (basically, not doing any feature extraction and just supplying the input image to the classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hsv_histogram(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    1. Resize the image to target_img_size using cv2.resize\n",
    "    2. Convert the image from BGR representation (cv2 is BGR not RGB) to HSV using cv2.cvtColor\n",
    "    3. Acquire the histogram using the cv2.calcHist. Apply the functions on the 3 channels. For the bins \n",
    "        parameter pass (8, 8, 8). For the ranges parameter pass ([0, 180, 0, 256, 0, 256]). Name the histogram\n",
    "        <hist>.\n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.resize(img,target_img_size)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv],[0,1,2],None,(8,8,8),[0,180,0,256,0,256])\n",
    "    \n",
    "    if imutils.is_cv2():\n",
    "        hist = cv2.normalize(hist)\n",
    "    else:\n",
    "        cv2.normalize(hist, hist)\n",
    "    return hist.flatten()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    You won't implement anything in this function. You just need to understand it \n",
    "    and understand its parameters (i.e win_size, cell_size, ... etc)\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, target_img_size)\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    \n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    h = h.flatten()\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_pixels(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    The classification algorithms we are going to use expect the input to be a vector not a matrix. \n",
    "    This is because they are general purpose and don't work only on images.\n",
    "    CNNs, on the other hand, expect matrices since they operate on images and exploit the \n",
    "    arrangement of pixels in the 2-D space.\n",
    "    \n",
    "    So, what we only need to do in this function is to resize and flatten the image.\n",
    "    \"\"\"\n",
    "    img=cv2.resize(img, target_img_size)\n",
    "    return img.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img, feature_set='hog'):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Given either 'hsv_hist', 'hog', 'raw', call the respective function and return its output\n",
    "    \"\"\"\n",
    "    if feature_set == 'hog':\n",
    "        return extract_hog_features(img)\n",
    "    elif feature_set =='raw':\n",
    "        return extract_raw_pixels (img)\n",
    "    else: return extract_hsv_histogram(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will extract the features and the label of each image in our dataset and save it in RAM. We normally don't save datasets in RAM, but this dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(feature_set='hog'):\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_filenames = os.listdir(path_to_dataset)\n",
    "\n",
    "    for i, fn in enumerate(img_filenames):\n",
    "        if fn.split('.')[-1] != 'jpg':\n",
    "            continue\n",
    "\n",
    "        label = fn.split('.')[0]\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_dataset, fn)\n",
    "        img = cv2.imread(path)\n",
    "        features.append(extract_features(img, feature_set))\n",
    "        \n",
    "        # show an update every 1,000 images\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames)))\n",
    "        \n",
    "    return features, labels        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Classification\n",
    "\n",
    "In this part, we will test the classification performance of SVM, KNN, & NNs given our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO understand the hyperparameters of each classifier\n",
    "classifiers = {\n",
    "    'SVM': svm.LinearSVC(random_state=random_seed),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7),\n",
    "    'NN': MLPClassifier(solver='sgd', random_state=random_seed, hidden_layer_sizes=(500,), max_iter=20, verbose=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will test all our classifiers on a specific feature set\n",
    "def run_experiment(feature_set):\n",
    "    \n",
    "    # Load dataset with extracted features\n",
    "    print('Loading dataset. This will take time ...')\n",
    "    features, labels = load_dataset(feature_set)\n",
    "    print('Finished loading dataset.')\n",
    "    \n",
    "    # Since we don't want to know the performance of our classifier on images it has seen before\n",
    "    # we are going to withhold some images that we will test the classifier on after training \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    for model_name, model in classifiers.items():\n",
    "        print('############## Training', model_name, \"##############\")\n",
    "        # Train the model only on the training features\n",
    "        model.fit(train_features, train_labels)\n",
    "        \n",
    "        # Test the model on images it hasn't seen before\n",
    "        accuracy = model.score(test_features, test_labels)\n",
    "        \n",
    "        print(model_name, 'accuracy:', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see how each classifier and each feature set performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] processed 1000/7200\n",
      "[INFO] processed 2000/7200\n",
      "[INFO] processed 3000/7200\n",
      "[INFO] processed 4000/7200\n",
      "[INFO] processed 5000/7200\n",
      "[INFO] processed 6000/7200\n",
      "[INFO] processed 7000/7200\n",
      "Finished loading dataset.\n",
      "############## Training SVM ##############\n",
      "SVM accuracy: 97.70833333333333 %\n",
      "############## Training KNN ##############\n",
      "KNN accuracy: 96.52777777777779 %\n",
      "############## Training NN ##############\n",
      "Iteration 1, loss = 2.15704584\n",
      "Iteration 2, loss = 1.99560963\n",
      "Iteration 3, loss = 1.83560058\n",
      "Iteration 4, loss = 1.68165879\n",
      "Iteration 5, loss = 1.53364032\n",
      "Iteration 6, loss = 1.39346492\n",
      "Iteration 7, loss = 1.26395924\n",
      "Iteration 8, loss = 1.14686978\n",
      "Iteration 9, loss = 1.04253337\n",
      "Iteration 10, loss = 0.95068295\n",
      "Iteration 11, loss = 0.87050184\n",
      "Iteration 12, loss = 0.80047147\n",
      "Iteration 13, loss = 0.73942031\n",
      "Iteration 14, loss = 0.68650635\n",
      "Iteration 15, loss = 0.64004929\n",
      "Iteration 16, loss = 0.59965613\n",
      "Iteration 17, loss = 0.56371149\n",
      "Iteration 18, loss = 0.53196620\n",
      "Iteration 19, loss = 0.50386822\n",
      "Iteration 20, loss = 0.47872302\n",
      "NN accuracy: 93.95833333333333 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\premi\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou should get the following test accuracies the first time \\n\\nSVM accuracy ~ 97.70833333333333\\nKNN accuracy ~ 96.52777777777779\\nNN accuracy ~ 93.95833333333333\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment('hog')\n",
    "\"\"\"\n",
    "You should get the following test accuracies the first time \n",
    "\n",
    "SVM accuracy ~ 97.70833333333333\n",
    "KNN accuracy ~ 96.52777777777779\n",
    "NN accuracy ~ 93.95833333333333\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] processed 1000/7200\n",
      "[INFO] processed 2000/7200\n",
      "[INFO] processed 3000/7200\n",
      "[INFO] processed 4000/7200\n",
      "[INFO] processed 5000/7200\n",
      "[INFO] processed 6000/7200\n",
      "[INFO] processed 7000/7200\n",
      "Finished loading dataset.\n",
      "############## Training SVM ##############\n",
      "SVM accuracy: 32.013888888888886 %\n",
      "############## Training KNN ##############\n",
      "KNN accuracy: 32.708333333333336 %\n",
      "############## Training NN ##############\n",
      "Iteration 1, loss = 2.20297167\n",
      "Iteration 2, loss = 2.20170845\n",
      "Iteration 3, loss = 2.20062078\n",
      "Iteration 4, loss = 2.19978368\n",
      "Iteration 5, loss = 2.19912285\n",
      "Iteration 6, loss = 2.19865292\n",
      "Iteration 7, loss = 2.19816292\n",
      "Iteration 8, loss = 2.19783041\n",
      "Iteration 9, loss = 2.19757067\n",
      "Iteration 10, loss = 2.19736311\n",
      "Iteration 11, loss = 2.19715509\n",
      "Iteration 12, loss = 2.19698755\n",
      "Iteration 13, loss = 2.19689621\n",
      "Iteration 14, loss = 2.19675860\n",
      "Iteration 15, loss = 2.19668040\n",
      "Iteration 16, loss = 2.19662019\n",
      "Iteration 17, loss = 2.19655556\n",
      "Iteration 18, loss = 2.19650088\n",
      "Iteration 19, loss = 2.19646341\n",
      "Iteration 20, loss = 2.19640426\n",
      "NN accuracy: 9.722222222222223 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\premi\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou should get the following test accuracies the first time \\n\\nSVM accuracy ~ 32.083333333333336\\nKNN accuracy ~ 32.708333333333336\\nNN accuracy ~ 9.722222222222223\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment('hsv_hist')\n",
    "\"\"\"\n",
    "You should get the following test accuracies the first time \n",
    "\n",
    "SVM accuracy ~ 32.083333333333336\n",
    "KNN accuracy ~ 32.708333333333336\n",
    "NN accuracy ~ 9.722222222222223\n",
    "\"\"\"\n",
    "\n",
    "# Why low accuracies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] processed 1000/7200\n",
      "[INFO] processed 2000/7200\n",
      "[INFO] processed 3000/7200\n",
      "[INFO] processed 4000/7200\n",
      "[INFO] processed 5000/7200\n",
      "[INFO] processed 6000/7200\n",
      "[INFO] processed 7000/7200\n",
      "Finished loading dataset.\n",
      "############## Training SVM ##############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\premi\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: 83.125 %\n",
      "############## Training KNN ##############\n",
      "KNN accuracy: 93.95833333333333 %\n",
      "############## Training NN ##############\n",
      "Iteration 1, loss = 9.85298309\n",
      "Iteration 2, loss = 1.53641950\n",
      "Iteration 3, loss = 1.39441295\n",
      "Iteration 4, loss = 1.22810570\n",
      "Iteration 5, loss = 1.09824867\n",
      "Iteration 6, loss = 0.98433061\n",
      "Iteration 7, loss = 0.95905031\n",
      "Iteration 8, loss = 0.82031771\n",
      "Iteration 9, loss = 0.75031225\n",
      "Iteration 10, loss = 0.48351469\n",
      "Iteration 11, loss = 0.38983897\n",
      "Iteration 12, loss = 0.33997461\n",
      "Iteration 13, loss = 0.30691428\n",
      "Iteration 14, loss = 0.26990181\n",
      "Iteration 15, loss = 0.27333670\n",
      "Iteration 16, loss = 0.24701963\n",
      "Iteration 17, loss = 0.22850384\n",
      "Iteration 18, loss = 0.21095166\n",
      "Iteration 19, loss = 0.21062447\n",
      "Iteration 20, loss = 0.19232659\n",
      "NN accuracy: 88.68055555555556 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\premi\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou should get the following test accuracies the first time \\n\\nSVM accuracy ~ 85.06944444444444\\nKNN accuracy ~ 93.95833333333333\\nNN accuracy ~ 88.68055555555556\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment('raw')\n",
    "\"\"\"\n",
    "You should get the following test accuracies the first time \n",
    "\n",
    "SVM accuracy ~ 85.06944444444444\n",
    "KNN accuracy ~ 93.95833333333333\n",
    "NN accuracy ~ 88.68055555555556\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers list now has models trained on the last feature set you ran an experiment on. You can play around with it checking the probability it gives to each label, given an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "test_img_path = r'test2.jpg'\n",
    "img = cv2.imread(test_img_path)\n",
    "features = extract_features(img, 'raw')  # be careful of the choice of feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.30439737e-08, 9.93205801e-01, 6.76568859e-03, 8.76096556e-12,\n",
       "        6.11874628e-07, 1.85395869e-08, 2.78609949e-05, 1.20615934e-09,\n",
       "        4.52127110e-09]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = classifiers['NN']\n",
    "nn.predict_proba([features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get a better accuracy by changing the model hyperparameters and retraining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
